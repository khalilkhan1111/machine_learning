# machine_learning

Machine learning is a subfield of artificial intelligence (AI) that focuses on the development of algorithms and models that enable computers to learn and make predictions or decisions without being explicitly programmed.

Machine learning can be categorized into several types based on the learning approach and the nature of the training data. The three main types of machine learning are:

Supervised Learning:

In supervised learning, the model is trained on a labeled dataset, where each input data point is associated with a corresponding output or target.

The goal is for the model to learn a mapping from inputs to outputs, allowing it to make predictions or classifications on new, unseen data.

Common algorithms used in supervised learning include linear regression, logistic regression, decision trees, support vector machines, and various types of neural networks.

Unsupervised Learning:

Unsupervised learning involves training models on unlabeled data, where the algorithm tries to discover patterns, structures, or relationships within the data.

It is often used for tasks such as clustering (grouping similar data points together) and dimensionality reduction (reducing the number of features while retaining important information).

Common unsupervised learning algorithms include k-means clustering, hierarchical clustering, principal component analysis (PCA), and autoencoders.

Reinforcement Learning:

Reinforcement learning is concerned with training agents to make a sequence of decisions in an environment to maximize a cumulative reward.

The agent learns through trial and error, adjusting its actions based on the rewards it receives from the environment.

Linear Regression:

Linear regression is a fundamental statistical and machine learning technique used for modeling the relationship between a dependent variable (also called the target) and one or more independent variables (predictors or features). It assumes that this relationship is approximately linear, which means that the change in the dependent variable is proportional to changes in the independent variables.

Y = mx + c

where 
y: Dependent variable

x: Independent variable

m : slope of line

c: constant

Best fit line:

Determining the best-fit line in linear regression involves finding the line that minimizes the sum of the squared differences (residuals) between the observed data points and the predicted values generated by the line. To determine best fit line we use sum of squared error and formulae is (actual output - predicted output)**2

Evaluation metrices:

Mean Absolute Error (MAE): MAE calculates the average absolute difference between predicted and actual values. It is robust to outliers. 
Formula is MAE = (1/n) * sum(|y_i - y_pred_i|)

n is the number of data points

y_i is the actual value for the i-th data point

y_pred_i is the predicted value for the i-th data point

Mean Squared Error (MSE): MSE calculates the average squared difference between predicted and actual values. It penalizes larger errors more heavily than MAE.
Formula is (1/n) * sum((y_i - y_pred_i)^2)

n is the number of data points

y_i is the actual value for the i-th data point

y_pred_i is the predicted value for the i-th data point

Root Mean Squared Error (RMSE): RMSE is the square root of MSE and provides an interpretable metric in the same unit as the target variable.
Formula is  RMSE = sqrt(MSE)

MSE is the mean squared error

R-squared (R**2): R-squared measures the proportion of the variance in the dependent variable that is explained by the model. It ranges from 0 to 1, where higher values indicate a better fit.
Formula is R^2 = 1 - (SSR / SST)

SSR is the sum of squared residuals

SST is the sum of squared totals

SSR is the sum of the squared differences between the actual and predicted values for each data point. SST is the sum of the squared differences between each data point and the mean of all data points.

Mean Absolute Percentage Error (MAPE): MAPE calculates the average percentage difference between predicted and actual values. It is often used in forecasting problems.
formula is MAPE = (1/n) * sum(|(y_i - y_pred_i) / y_i| * 100)

n is the number of data points

y_i is the actual value for the i-th data point

y_pred_i is the predicted value for the i-th data point

Homoscedasticity:

Homoscedasticity is a statistical property of data that means that the variance of the errors is constant across all values of the independent variable. This means that the error terms are equally spread out around the regression line, regardless of the value of the independent variable.

Heteroscedasticity:

Heteroscedasticity is a statistical property of data that means that the variance of the errors is not constant across all values of the independent variable. This means that the error terms are not equally spread out around the regression line, regardless of the value of the independent variable.

Heteroscedasticity can be caused by a number of factors, such as outliers, non-linear relationships between the independent and dependent variables, and measurement error.

Properties of regression lines:

A regression line always passes through the mean of the dependent variable. This is because the regression line minimizes the sum of the squared residuals, and the mean of the dependent variable is the point that minimizes the sum of the squared residuals.

The slope of the regression line indicates the direction and strength of the relationship between the independent and dependent variables. A positive slope indicates that the independent variable and dependent variable are positively correlated, meaning that as the independent variable increases, the dependent variable also increases. A negative slope indicates that the independent variable and dependent variable are negatively correlated, meaning that as the independent variable increases, the dependent variable decreases.

The intercept of the regression line indicates the value of the dependent variable when the independent variable is equal to zero. This value is often not meaningful, but it can be useful for interpreting the regression line.

The regression line does not perfectly fit the data. There will always be some error between the predicted values and the actual values.

The regression line can be used to make predictions about the dependent variable for new values of the independent variable. However, it is important to remember that these predictions are only estimates, and they may not be accurate for all values of the independent variable.

Advantages of linear regressions:

Simple to understand and interpret.

Easy to implement.

Robust to noise.

Versatile.

Limitations:

Assumptions: Linear regression models make a number of assumptions about the data, such as normality, homoscedasticity, and independence of the errors. If these assumptions are not met, then the model may not be accurate.

Outliers.

Multicollinearity

Overfitting.

Cross-validation is a technique for evaluating the performance of a machine learning model on unseen data. It works by splitting the training data into a number of folds, and then training and evaluating the model on each fold. The average performance of the model on the folds is then used to assess its overall performance.

Hyperparameter tuning is the process of finding the optimal values for the hyperparameters of a machine learning model. Hyperparameters are parameters that control the learning process of the model, but they are not learned from the data.

Here are some of the benefits of using cross-validation and hyperparameter tuning:

Improved model performance: Cross-validation and hyperparameter tuning can help you to find the best model hyperparameters, which can lead to improved model performance.
Reduced overfitting: Cross-validation can help you to identify and avoid overfitting, which is a problem where the model fits the training data too closely and does not generalize well to new data.

More reliable model selection: Cross-validation can help you to select the best model from a set of candidate models, which can lead to more reliable model selection.
